---
title: gcp_bigquery
type: output
status: experimental
categories: ["Services","GCP"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the contents of:
     lib/output/gcp_bigquery.go
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution EXPERIMENTAL
This component is experimental and therefore subject to change or removal outside of major version releases.
:::

Sends message parts as new rows to a Google Cloud BigQuery table. Currently json is the only supported format.
Each object is stored in the dataset and table specified with the `dataset` and `table` fields.

Introduced in version 3.52.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yaml
# Common config fields, showing default values
output:
  label: ""
  gcp_bigquery:
    project: ""
    dataset: ""
    table: ""
    format: NEWLINE_DELIMITED_JSON
    max_in_flight: 1
    csv:
      header: []
      field_delimiter: ','
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

</TabItem>
<TabItem value="advanced">

```yaml
# All config fields, showing default values
output:
  label: ""
  gcp_bigquery:
    project: ""
    dataset: ""
    table: ""
    format: NEWLINE_DELIMITED_JSON
    max_in_flight: 1
    write_disposition: WRITE_APPEND
    create_disposition: CREATE_IF_NEEDED
    ignore_unknown_values: false
    max_bad_records: 0
    auto_detect: false
    csv:
      header: []
      field_delimiter: ','
      allow_jagged_rows: false
      allow_quoted_newlines: false
      encoding: UTF-8
      skip_leading_rows: 1
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: []
```

</TabItem>
</Tabs>

### Credentials

By default Benthos will use a shared credentials file when connecting to GCP
services. You can find out more [in this document](/docs/guides/gcp).

### Dataset and Table

Currently this plugin cannot create a new Dataset nor a new Table, where both need
to exist for this output to be used.

### Format

Currently this plugins supports only CSV and JSON formats.
Learn more about how GCP BigQuery deals with these formats here:
- [`JSON`](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json)
- [`CSV`](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv)

Each message may contains multiple single-line elements separated by a \n.

For example a single message containing
```json
{"key" : "1"}
{"key" : "2"}
```

and multiples messages containing
```json
{"key" : "1"}
```

```json
{"key" : "2"}
```

Will both result in two rows inside the BigQuery. The same is valid for the CSV format.

Example of invalid message:
```json
{
	"key" : "2"
}
```

#### CSV

For the CSV format the field `csv.header` makes benthos add the header as the first line in each batch using the `csv.field_delimiter` as delimiter to send data to Google Cloud BigQuery.
If this field is not provided, the first message in the output batch will be used as header.


## Performance

This output benefits from sending multiple messages in flight in parallel for
improved performance. You can tune the max number of in flight messages with the
field `max_in_flight`.

This output benefits from sending messages as a batch for improved performance.
Batches can be formed at both the input and output level. You can find out more
[in this doc](/docs/configuration/batching).

## Fields

### `project`

The project ID of the dataset to insert data to.


Type: `string`  
Default: `""`  

### `dataset`

BigQuery Dataset Id. Do not include project id in this field.


Type: `string`  
Default: `""`  

### `table`

The table to insert messages to.


Type: `string`  
Default: `""`  

### `format`

The format of each incoming message.


Type: `string`  
Default: `"NEWLINE_DELIMITED_JSON"`  
Options: `NEWLINE_DELIMITED_JSON`, `CSV`.

### `max_in_flight`

The maximum number of messages to have in flight at a given time. Increase this to improve throughput.


Type: `int`  
Default: `1`  

### `write_disposition`

Specifies how existing data in a destination table is treated.


Type: `string`  
Default: `"WRITE_APPEND"`  
Options: `WRITE_APPEND`, `WRITE_EMPTY`, `WRITE_TRUNCATE`.

### `create_disposition`

Specifies the circumstances under which destination table will be created. If CREATE_IF_NEEDED is used the GCP BigQuery will create the table if it does not already exist and tables are created atomically on successful completion of a job. The CREATE_NEVER option ensures the table must already exist and will not be automatically created.


Type: `string`  
Default: `"CREATE_IF_NEEDED"`  
Options: `CREATE_IF_NEEDED`, `CREATE_NEVER`.

### `ignore_unknown_values`

Causes values not matching the schema to be tolerated. Unknown values are ignored. For CSV this ignores extra values at the end of a line. For JSON this ignores named values that do not match any column name. If this field is set to false (the default value), records containing unknown values are treated as bad records. The max_bad_records field can be used to customize how bad records are handled.


Type: `bool`  
Default: `false`  
Options: `true`, `false`.

### `max_bad_records`

The maximum number of bad records that will be ignored when reading data.


Type: `int`  
Default: `0`  

### `auto_detect`

Indicates if we should automatically infer the options and schema for CSV and JSON sources. If the table doesn't exists and this field is set to false the output may not be able to insert data and will throw insertion error. Be careful using this field since it delegates to the GCP BigQuery service the schema detection and values like "no" may be treated as booleans for the CSV format. You should probably create the table manually and leave this unset.


Type: `bool`  
Default: `false`  
Options: `true`, `false`.

### `csv`

Configurations used in the CSV format.


Type: `object`  

### `csv.header`

A list of values to use as header for each batch of messages. If not specified the first line of each message will be used as header. You should not enable batching if this field is not specified.


Type: `array`  
Default: `[]`  

### `csv.field_delimiter`

The separator for fields in a CSV file, used when reading or exporting data.


Type: `string`  
Default: `","`  

### `csv.allow_jagged_rows`

Causes missing trailing optional columns to be tolerated when reading CSV data. Missing values are treated as nulls.


Type: `bool`  
Default: `false`  
Options: `true`, `false`.

### `csv.allow_quoted_newlines`

Sets whether quoted data sections containing newlines are allowed when reading CSV data.


Type: `bool`  
Default: `false`  
Options: `true`, `false`.

### `csv.encoding`

Encoding is the character encoding of data to be read.


Type: `string`  
Default: `"UTF-8"`  
Options: `UTF-8`, `ISO-8859-1`.

### `csv.skip_leading_rows`

The number of rows at the top of a CSV file that BigQuery will skip when reading data. The default value is 1 since Benthos will add the specified header in the first line of each batch sent to BigQuery.


Type: `int`  
Default: `1`  

### `batching`

Allows you to configure a [batching policy](/docs/configuration/batching).


Type: `object`  

```yaml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
```

### `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


Type: `int`  
Default: `0`  

### `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


Type: `int`  
Default: `0`  

### `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


Type: `string`  
Default: `""`  

```yaml
# Examples

period: 1s

period: 1m

period: 500ms
```

### `batching.check`

A [Bloblang query](/docs/guides/bloblang/about/) that should return a boolean value indicating whether a message should end a batch.


Type: `string`  
Default: `""`  

```yaml
# Examples

check: this.type == "end_of_transaction"
```

### `batching.processors`

A list of [processors](/docs/components/processors/about) to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


Type: `array`  
Default: `[]`  

```yaml
# Examples

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array

processors:
  - merge_json: {}
```


